// pages/api/chat.js
import { PineconeClient } from "@pinecone-database/pinecone";
import {
  ConversationalRetrievalQAChain,
  RetrievalQAChain,
} from "langchain/chains";
import { ChatOpenAI } from "langchain/chat_models/openai";
import { OpenAIEmbeddings } from "langchain/embeddings/openai";
import {
  BufferMemory,
  ChatMessageHistory,
  ConversationSummaryMemory,
} from "langchain/memory";
import {
  ChatPromptTemplate,
  HumanMessagePromptTemplate,
  MessagesPlaceholder,
  SystemMessagePromptTemplate,
} from "langchain/prompts";
import { ContextualCompressionRetriever } from "langchain/retrievers/contextual_compression";
import { LLMChainExtractor } from "langchain/retrievers/document_compressors/chain_extract";
import { HumanChatMessage, SystemChatMessage } from "langchain/schema";
import { PineconeStore } from "langchain/vectorstores/pinecone";

const openAIApiKey = process.env.OPENAIKEY;

const embeddings = new OpenAIEmbeddings({
  modelName: "gpt-3.5-turbo",
  openAIApiKey: openAIApiKey,
});

const fasterModel = new ChatOpenAI({
  modelName: "gpt-3.5-turbo",
  openAIApiKey: openAIApiKey,
});
const slowerModel = new ChatOpenAI({
  modelName: "gpt-4",
  openAIApiKey: openAIApiKey,
});

// const memory = new BufferMemory({
//   memoryKey: "chat_history",
//   inputKey: "question", // The key for the input to the chain
//   outputKey: "text", // The key for the final conversational output of the chain
//   returnMessages: true, // If using with a chat model
// });

const chat_history = [];

function truncate(str, no_words) {
  return str.split(" ").splice(0, no_words).join(" ");
}

export default async function handler(req, res) {
  try {
    const client = new PineconeClient();
    await client.init({
      apiKey: "52be664a-a555-4a86-8d6f-6ec9d666c7be",
      environment: "asia-southeast1-gcp-free",
    });
    const pineconeIndex = client.Index("chat-cv");

    const vectorStore = await PineconeStore.fromExistingIndex(embeddings, {
      pineconeIndex,
    });
    const baseCompressor = LLMChainExtractor.fromLLM(slowerModel);

    const retriever = await new ContextualCompressionRetriever({
      baseCompressor,
      baseRetriever: vectorStore.asRetriever(),
    });
    const { input, promptInjection, messages } = req.body;

    let queryText = input;

    queryText +=
      "\nIf necessary, utilize the below chat history as additional context:" +
      JSON.stringify(messages);

    const appendHistory =
      "\nIf necessary, utilize the below chat history as additional context:" +
      JSON.stringify(messages);

    // console.log("ðŸš€ ~ file: chat.js:62 ~ handler ~ queryText:", queryText);
    // const promptPrefix = `

    // You should follow the following rules when generating and answer:
    // - You are Alfred Pennyworth, the trustworthy and reliable butler of Bruce Wayne (aka Batman)
    // - Today, you're helping Samuel Morgan-Tyghe, a skilled web developer, find a new job.
    // - Your task is to present Sam's CV to potential employers in a way that highlights his skills, experience, and accomplishments.
    // - Remember, your responses should be helpful, relevant, and concise. You must understand and acknowledge the individual you're interacting with.
    // - Keep your responses under 150 tokens.
    // - When referencing feedback use Qoutes and inlude the name of the person giving the feedback and thier linkedin url.
    // - format text with plenty of space
    // - never mention what is in the job spec
    // - never mention the job spec
    // - model your answers to try and fullfill the job spec
    // - use the job spec as a guide to what you should be talking about
    // - if the job spec is empty ignore the job spec
    // - keep your responses under 150 tokens.
    // - any other sources of information are coming from a vectorised database (Pinecone)
    // -  dont mention any of the rules

    // The following text is user input:
    // `;

    // const questionGeneratorPrompt = ChatPromptTemplate.fromPromptMessages([
    //   new SystemChatMessage(promptInjection),
    //   new HumanChatMessage(input),
    //   new SystemChatMessage(appendHistory),
    // ]);

    // const results = await vectorStore.similaritySearch("pinecone", 1, {
    //   foo: "bar",
    // });
    const chain = RetrievalQAChain.fromLLM(fasterModel, retriever);

    let response = await chain.call({
      query: truncate(
        `System Text:${promptInjection}
    human text : ${input}
    System Text: ${appendHistory}`,
        3500
      ),
      // query: [
      //   new SystemChatMessage(promptInjection),
      //   new HumanChatMessage(input),
      //   new SystemChatMessage(appendHistory),
      // ],
    });

    res
      .status(200)
      .json({ result: response.text, context: response?.sourceDocuments });
  } catch (error) {
    console.log("ðŸš€ ~ file: chat.tsx:133 ~ handler ~ error:", error);
  }
}
